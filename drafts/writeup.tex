
\documentclass{article}

\begin{document}

\section*{Introduction}
What enables humans to learn -- from a finite number of examples --
systems of concepts that contain a potentially unbounded number of
individuals elements? 

How do people learn to combine new conceptual knowledge with core knowledge?

Is the acquisiton of conceptual knowledge driven by generalization
from examples or by the application of high-level conceptual schemas?

Does the structure of language or the learning of language guide
the learning of concepts? 

How does one learn what counts as evidence of conceptual meaning?  

What is the nature of conceptual knowledge? Is it declarative, a
collection of facts from which questions are answered, best actions
are inferred, etc.? Or is it procedural, a collection of routines or
productions? Or both? If both, how do these two forms of knowledge
interact? 

\section*{AAAI Abstract}
Title: Learning the many meanings of number words. 
or   : Grammar induction for multi-faceted concept learning. 

Title: Scaffolding meaning on unsupervised grammar induction.

What comes first, learning the structure of a perceptual input or
learning its meaning? Evidence from psychology and linguistics
suggests that children jointly learn the compositional structure of
words and phrases and how to map meanings onto those structural
components. But little is known about whether jointly learn structure
and meaning can improve AI and machine learning performance, because
most previous research has focused on supervised learning (mapping
perceptual inputs directly to task-relevant outputs) or unsupervised
learning (asserting that the learned structure *is* the output). Here,
we combine probabilistic grammar induction with supervised learning

When does someone *understand* a concept? Intuitively, understanding a
concept requires more than being able to answer a specific query
(e.g. is an element a member of that concept). Rather, we say that a
person understands a concept when they are able to leverage their
knowledge of that concept to perform a wide set of (perhaps
unspecified) tasks. Take, for example, a child's knowledge of number,
and the various kinds of queries that a mastery of the concept
ultimately supports support: counting (of various kinds, up, down, by
fives); comparing numbers (which is greater?); approximate numerical
understanding (is ten approximately half of a hundred? how about forty
five?); converting numbers to digits (e.g. ``four hundred and five''
to 405)l; converting digits to numbers (e.g. 1243 to ``twelve hundred
and forty three''); and an unbounded set of conceptual queries
(e.g. Is their a largest/smallest number? If I take N marbles out of a
bowl and put N back in, are there more, less, or the same number of
marbles in the bowl? If I start at five and start counting up by one
will I every encounter the same number twice? What's one more than a
plutpyltillion and one?).

What would it take for computers to able to learn a similar kind of
conceptual understanding? One possibility is to attempt to learn ev to
answer every query independently. At the other extreme, typified by
inductive logic programming, the goal is to learn a coplletel theory
of the domain of interest in the form a logical theory, such that
queries are expressed as querying of a logical thoery. However, But it
is more But people's actual competency is somewhere in between these
two approaches.  Clearly, people can answer arbitrary new
questions. But they cannot perform any manipultation. Fro example,
people cannot cannot add arbitrarily large numbers in their
heads. People don't know whether there ais a largest prime.
Therefore, we propose an intermediate kgoalswhich is that people learn
what will term ``conceptual routines''. This might include counting,
comparing numbers, approximate magnitutde routines, etc. And that
these are each learned on a the basis of common representation. We
learn this base repsenetation in an unsupervised manner, and learn
these conceptual routines on top of the learned representation. 

In this paper, we focus on a subset of the kinds of queries How can
computers learn a similar kind of conceptual understanding as this?
Although Psychological and linguistic investigations indicate that
children's concept of number develops over many years.

-----------------------------------------------------------------

Title: Domain specific grammar induction for semi-supervised
programming by demonstration.

Abstract: For programming by demonstration (PDB) systems to be useful
to non-expert users, these systems must be able to understand and
manipulate data that is implicitly structured in a domain-specific
way. For example, the data might be email addresses or street
addresses, a company-specific record format for book-keeping, an ad
hoc organization relying on visual separation for demaraction, or a
user's shorthand notation. While recent success in PBD has been driven
by the careful construction of domain-specific languages, those
languages generally have fixed or user-specified primitives and
features. Here, we show that a hierarchical Bayesian approach to
grammar induction can be used as a general and robust tool for
learning the latent structure of user data, and show that such
learning can help automate tasks that would otherwise require
task-specific programming to solve.

------------------------------------------------------------

One of the distinguishing aspects of human societies is their use of
symbol systems to express useful systems of concepts. One particularly
successful such symbol system is the Arabic numerals, which are used
to represent things as varied as sizes, lengths, times, dates, and
cardinality. What is it that makes this a successful symbol system for
these purposes, and what is it that enables people to learn the
various meanings of these symbols from a small fraction of the
infinite possible elements? Our proposal is that what makes this a
successful symbol system is that it bridges the gap between important
and innate domains of semantics, i.e., the compositional semantics of
language and the geometric semantics of our core cognition of
magnitude, space and time. We provide a model that is based on this
proposal. The use of the Arabic numerals provides the model access to
both kinds of semantic representations, and allow the model to learn
the mappings that would otherwise require many more examples or would
eb impossible.

\end{document}