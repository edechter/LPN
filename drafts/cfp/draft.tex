\documentclass[10pt, twocolumn]{article}

\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{anyfontsize}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{array}
%\usepackage{multicol}

\setlength{\droptitle}{-4em}   % This is your set screw

\title{Latent Predicate Networks: Concept Learning with Probabilistic Context-Sensitive Grammars}
\author{Eyal Dechter,  Josh Rule, Josh Tenenbaum}
\date{}

\begin{document}
\vspace{-10cm}
\maketitle
\vspace{-2em}

{\small For humans, learning abstract concepts and learning language
  go hand in hand: we acquire abstract knowledge primarily through
  linguistic experience, and acquiring abstract concepts is a crucial
  step in learning the meanings of linguistic expressions. Number
  knowledge is a case in point: we largely acquire concepts such as
  seventy-three through linguistic means, and we can only know what
  the sentence ``Seventy-three is more than twice as big as
  thirty-one" means if we can grasp the meanings of its component
  number words. How do we make sense of this potential paradox? One
  approach is to estimate the distribution from which sentences
  are drawn, and, in doing so, infer the latent concepts and
  relationships that best explain those sentences. We present early work on a
  learning framework called Latent Predicate Networks (LPNs) which
  learns concepts by inferring the parameters of probabilistic
  context-sensitive grammars over sentences.  We show that for a small
  fragment of sentences expressing relationships between English
  number words, we can use hierarchical Bayesian inference to learn
  LPNs that can answer simple queries about previously unseen
  abstract concepts within this domain. These generalizations
  demonstrate LPNs' promise as a tool for learning and representing
  conceptual knowledge in language.}

\section{Introduction}

Although concept learning and language acquisition have typically been
treated as distinct problems in AI, linguistics and cognitive
development, for a child learning to understand language, they are
strongly coupled. People learn about many abstract concepts primarily
through language and understanding language depends on understanding
concepts.  Research in concept learning is often focused on concepts
grounded in perceptual features, and while it is probably true that
many concepts are learned via generalization from examples, some
concepts cannot possibly be learned this way.

Number concepts are a good example: children do not learn about the
meaning of ``seventy five'' by seeing examples of seventy five things;
they do not know that ``seventy five'' is more than ``twenty five''
because of their perceptual experiences of these quantities. Rather,
children learn the meaning of ``seventy five'' (or ``a billion and
five'') by noticing how number words are used in language, in counting
sequences, in arithmetic excercises, etc. Other good examples of such
abstract concepts are kinship and social relations (e.g. ``my father
in law's grandmother''), temporal relations (``the day after last
Thanksgiving.''), and spatial relations (``above and just to the left
of'').

Such abstract concepts share many of the properties of language
syntax: there is an unbounded set of them, they derive their
meanings via composition, and, although people only ever say, hear, or
read about a small number of them, they are able to reason correctly
about any them. There seems to be a grammar to these concepts, and
grasping this grammar is critical to understanding their meanings and
how to use them. This motivates our approach here, which is to apply
the tools of probabilistic grammars more familiar from studies of
syntax to the problem of concept aquisition.

Doing so requires overcoming some technical barriers. 

First, whereas context-free grammars are suitable for describing large
swaths of language syntax, the grammars of concepts are not
context-free. To address this, we use Range Concatentation Grammars, a
context-sensitive grammar formalism -- one of several developed within
linguistics -- and extend this formalism to a probabilistic model of
sentences. 

Second, the categories of syntax -- the nonterminals of the grammar --
are often assumed to be known to children innately and given to
automated learners by human experts. The categories that underlie
conceptual knowledge, on the other hand, are far more numerous, vary
from domain to domain, and are unlikely to be known to the
learner. This motivates our use of latent predicates that, through
learning, assume the role of a domain's underlying concepts (in the
number domain, these might correspond to the concepts of
successorship, order of magnitude, magnitude comparison,
exact vs. approximate, etc.).

Finally, using probabilistic context-sensitive grammars with latent
predicates brings up problems of tractability: our goal is to find a
middle ground between expressivity and tractability. Using tools like
PRISM -- a probabilistic logic programming system that naturally
implements efficient dynamic programming algorithms for our models --
we are able to explore which domains and which grammar architectures
are a good fit for this grammar-based approach.

In the rest of this paper describe early work on this approach. First, we
present our probabilistic model of concept learning, which we call
Latent Predicate Networks (LPNs). Then  we describe our approach to
inference and its implementation in PRISM. Finally, we present
preliminary experimental results in the domain of number concept
learning that demonstrate the promise of this approach. 

% people learn about many
% abstract concepts primarily through language and, conversely, these
% abstract concepts are critical to understanding language. Consider a
% child learning the meaning of the number words: the meaning of the
% ``seventy five'' is learned not by seeing collections of seventy five
% things but rather by understanding how ``seventy five'' is used in the
% various sentences, counting sequences, arithmetic excercises, etc.

% As Claude Shannon demonstrated with his
% guessing game experiment in 1950~\cite{Shannon1950}, an adult English
% speaker's understanding of her language and her world is such that the
% entropy of a character in English text is slightly more than one
% bit. Plausibly, any machine learning system able to predict natural
% language text with such accuracy will have learned much about the
% concepts expressed in that text. This intuition motivates us to invert
% Shannon's experiment and learn linguistic concepts by learning a
% probabilistic generative model for natural language.

% Probabilistic Context Free Grammars (PCFGs) are popular and much
% studied generative models for natural language
% syntax~\cite{Someone}. While debate continues about their ability to
% express the syntax of natural language, it is agreed that they cannot
% express dependencies between words that depend upon each other for
% their meaning. Consider an example from the domain of sentences
% addressed in this paper: ``after twenty eight comes twenty nine.''
% The second ``twenty'' depends on the first ``twenty'' and the
% ``eight'', and the ``nine'' depends on the ``eight''. This pattern is
% a clear example of the \emph{cross-serial dependencies} which
% context-free grammars cannot support.

% Our approach, then, will be to explore the use of probabilistic
% context \emph{sensitive} grammars for modeling natural language,
% extending the tools of hierarchical grammar induction~\cite{SOMEONE} from
% context-free grammars to ones that can express not only syntactic
% relationships but semantic ones as well. In the place of latent
% syntactic categories, we will refer to latent predicates -- relations
% between sentence components -- that, when learned, should correspond
% to the concepts that underlie sentence semantics.

% To explore this approach, we introduce a probabilisitic model called a
% Latent Predicate Network (LPN), and describe an inference procedure to
% learn its parameters using the probabilistic logic programming
% language PRISM. Finally, we study its performance when trained on
% sentences describing the relationships between numbers.

\section{Latent Predicate Networks}

An LPN is a hierarchical Bayesian model of strings
extending the Hierarchical Dirichlet PCFG model (HD-PCFG) to
Stochastic Range Concatenation Grammars (SRCGs). 

\subsection{Probabilistic Range Concatentation Grammars}
Range Concatenation Grammars (RCGs) are a class of string grammars
that represent all and only those languages that can be parsed in time
polynomial in the length of the target
string~\cite{boullier2005range}. An RCG $G=(N, T, V, P, S)$ is a
5-tuple where $N$ is a finite set of predicate symbols, $T$ is a set
of terminal symbols, $V$ is a set of variable symbols, P is a finite
set of $M \geq 0$ clauses of the form $\psi_0 \rightarrow \psi_1 \dots
\psi_M$, and $S \in N$ is the \emph{axiom}. Each $\psi_m$ is a term of
the form $A(\alpha_1, \dots, \alpha_{\mathcal{A}(A)}$, where $A \in
N$, $\mathcal{A}(A)$ is the arity of $A$, and each $\alpha_i \in (T
\cup V)^*$ is an argument of $\psi_m$. We call the left hand side term
of any clause the \emph{head} of that clause and its predicate symbol
is the \emph{head predicate}.

\textbf{Example:} The following is an RCG for the 3-copy language
$\{www | w \in \{a, b\}^*\}$:
\begin{alignat*}{3}
&S(XYZ) &&\rightarrow A(X, Y, Z)\\
&A(aX, aY, aZ) &&\rightarrow A(X, Y, Z)\\
&A(bX, bY, bZ) &&\rightarrow A(X, Y, Z)\\
&A(\epsilon, \epsilon, \epsilon) &&\rightarrow 
\end{alignat*}

A string $x$ is in the language defined by an RCG if one can
\emph{derive} $S(x)$. A derivation is a sequence of rewrite steps in
which substrings of the left hand side argument string are bound to
the variables of the head of some clause, thus determining the
arguments in the clause body. If a clause has no body terms, then its
head is derived; otherwise, its head is derived if its body clauses
are derived~\footnote{This description of the language of an RCG
  technically only holds for \emph{non-combinatory} RCGs, in which the
  arguments of body terms can only contain single variables. Since any
  \emph{combinatory} RCG can be converted into a non-combinatory RCG
  and we only consider non-combinatory RCGs here, this description
  suffices.}

We extend RCGs to SRCGs by annotating each clause $C_k \in P$ with
probabilities $p_k$ such that for all predicates ${A \in N, \,
  \sum_{k:head(C_k)=A} p_k = 1}$. An SRCG defines a distribution over
strings $x$ by sampling from derivations of $S(x)$ according to the
product of probabilities of clauses used in that derivation. This is a
well defined distribution as long as no probability mass is placed on
derivations of infinite length; in this paper, we only consider SRCGs
with derivations of finite length, so we need not worry about this
requirement.

\subsection{Architecture}

An LPN is an SRCG with 1 unary predicate $S$, and $K$ densely
connected binary predicates $A_1, \dots, A_K$ (Figure
\ref{fig:architecture}).  Specifically, for each latent predicate
$A_i$, we define rules such that $A_i(w_a,w_b)$ is true for each
possible pair of terminals, $w_a,w_b \in T$. We also define rules such
that $A_i(XY,UV)$ is true for each possible pair of predicates,
$A_j,A_k$, and every possible ordering of the variables $X,Y,U$ and
$V$ across $A_j$ and $A_k$. Finally, we define $S(XY)$ to be the
concatenation of two strings generated by $A_1$.
 
\begin{figure}[t]
		\includegraphics[width=\linewidth]{lpn/lpn.pdf}
		\caption{A schematic LPN.}
		\label{fig:architecture}
\end{figure}

% Now we can describe the model we introduce here, the Latent Concept
% Grammar (LCG). We introduce the LCG as a machine learning model to
% learn a distribution over instances of relations over strings. Unlike
% a PRCG, which places a distribution over terms RCGs consist of a set
% of rewrite rules, called clauses, each of which takes a termwhich
% operate on a set of terms (predicates applied to arguments) to produce
% new terms. Each rewrite rule is of the form:

% The vast majority of the models of concept acquisition that have been
% proposed in the artificial intelligence, machine learning and
% cognitive science literatures cannot represent the conceptual systems
% that even young children learn. Take, for example, the school-age
% child's knowledge of number. This child knows that there is no largest
% number; that ``one million forty three'' and ``8439''denote numbers
% but that ``one million forty billion'' and ``a82bd'' do not; that no
% two cardinalities refer to the same quantity but that some number
% words refer to the same cardinalities (e.g. ``two thousand'' and
% ``twenty-one hundred''). Moreover, the child knows how to guage the
% approximate number of elements in a collection and how to count this
% number exactly. In short, the child's knowledge of number consists of
% a complex system of concepts over an unbounded set of entities
% (i.e. the numbers). 

% Children are not born with this kind of knowledge, and, since number
% is a relatively recent cultural invention, nor is it likely that
% children possess innate domain specific background knowledge that
% makes it easier to acquire. Children only ever see a tiny fraction of
% all the numbers they can reason about and this evidence is biased,
% noisy, and can be unreliable. 

% A fundamental challenge for cognitive science -- and for any field
% interested in understanding human-like concept acquisition -- is to
% provide a theory of how such complex systems of concepts can be
% learned from sparse and noisy data.

% People are able to learn about complex systems of related
% concepts from small amounts of noisy data. A compelling approach to
% understanding such learning is to frame it as Bayesian inference over
% probabilistic knowledge bases. Advances in context free grammar
% induction in the past couple of decades can be seen as an instance of
% this approach, in which

% Our goal is to understand how children can obtain semantic knowledge
% about a system of concepts from the structure of relavent perceptual
% stimuli. How does a child learn that prefixing ``great-'' corresponds
% to adding an additional generation in kinship relationships? How does
% a child learn that ``just before'' is after ``a while before'' which
% is before ``a while after''? How does a child learn that if
% Figure~\ref{pairOfApples} is ``a pair of apples'' then
% Figure~\ref{singleApple} is a single apple? 

% Our proposal is that some interesting fragments of human conceptual
% knowledge can be represented as structured relations between
% linguistic tokens and other perceptual inputs; that the rules that
% define these relations implicitly define the concepts that are
% learned; and that these rules can be learned using the tools of
% hierarchical Bayesian inference. 

% For example, the concept TWENTY SEVEN is defined as the collection of
% representations ``27'', ``twenty seven'', ``3 times 9,'' along with a
% system of symbolic rules that translate among these representations
% and facilitate answered various queries. According to our hypothesis,
% the difference between TWENTY EIGHT and TWENTY SEVEN is not that they
% are represented in the mind by distinct mental symbols but rather that
% the rules that operate on their various representations treat them in
% systematically different ways.

% This view implies that an attempt to construct a computational account
% of concept learning in humans needs to ground out that learning in the
% relavent perceptual stimuli. 

% Contribution:
% \begin{itemize}
% \item We present Generate Rewrite Systems (GRS) -- a formalism that borrows
%   heavily from Linear Context-Free Rewriting Systems (CITE)
% \item We show how GRS, though restricted in representational capactiy,
%   can capture substantial cognitively plausible first-order conceptual knowledge 
% \item We propose a heirarchical Bayesian model for GRS that provides a coherent framework in which a GRS can be learned given evidence
% \item We show how GRS can be transformed into PRISM programs and learned within that probabilstic logic programming system
% \item We provide computational experiments that provide evidence that
%   our model can provide an account of some kinds of complex conceptual
%   knowledge can be inferred from noisy and sparse data
% \end{itemize}

% \section{The Model}

% Suppose we are given some positive instances $y_1, \dots,
% y_M$ of relations $R_1, \dots, R_N$. Our model assumes that each
% positive instance $y_m=R_n(\vec(x_m))$ is sampled i.i.d from a latent
% stochastic grammar (LSG), $\mathcal{K}$. $\mathcal{K}$ consists of
% some fixed number $K$ of clauses (i.e. rules) $C_1, \dots, C_K$ over
% the $N$ observable relations plus a fixed number of $H$ hidden
% relations $T_1, \dots, T_H$. The \emph{architecture} of the model is
% the exact specification of the parameters $K$ and $H$ and the clauses
% $\mathcal{C}$. To each clause $C_i$ is associated a positive real
% weights $\theta_i$.

% The clauses of $\mathcal{K}$ are Horn clauses in a first order logic
% with string concatenation. We will use PROLOG notation, in which lower
% case letters are predicates and constants and uppercase letters are
% variables. If two symbols are adjacent it means that the corresponding
% strings are concatenated. For example, the following would is a valid
% clause: $a(XY, XV) \leftarrow b(X, UV), a(X, X)$. In this rule $a$ and
% $b$ are relations with arity two.

% Let $C_1, \dots, C_{N(R)} = C(R)$ be the set of clauses whose head is
% relation $R$ and let $\Theta_R$ be the set of corresponding
% weights. Let $p_R(C_n) \sim \text{Discrete}(\Theta_R)$; that is, $p_R$
% is a distribution over the clauses headed by $R$, such that each
% clauses is sampled in proportion to its weight parameter.

% A positive instance of this relation is sampled from $\mathcal{K}$ by
% arbitrarily (i.e. uniformly at random) choosing a relation $R$ and
% instantiating that relation by sampling a clause from $p_R$. Recursing
% on this procedure for every atom in the body of this clause binds the
% variables in the clause's head. 

% Given this assumed generative process for the evidence, learning a
% program involves inferring the parameter $\theta$ given a prior
% probability distribution over their values. We place an independent
% Dirichlet distribution over the parameters of every relation. This is
% a natural choice that enables us to promote sparse programs, i.e.,
% programs in which most of the mass is placed on a small number of
% predicates and rules.

\subsection{Learning Model}

Given a collection of predicates, $\{A_k\}_{k=1}^{K}$, and a
distribution over clauses, $\{\vec{w}_{A_k}\}$, the learning task is
to model a set of utterances, $\{x_j\}_{j=1}^{J}$, as being generated
according to the following distribution:

\begin{align*}
  \vec{w}_{A_k} &\sim Dir(\vec{\alpha}_{A_k})\\
  x_j &\underset{iid}{\sim} p_{\text{\textsc{prcg}}}(S(x_j)\,|\,\{w_{A_k}\})
\end{align*}

\[ p(\{\vec{w}_{A_k}\}\,|\, \vec{x}, \{\vec{\alpha}_{A_k}\}) \propto\]\[ \prod_j
p_{\text{\textsc{prcg}}}(x_j|\{\vec{w}_{A_k}\}) \prod_{A_k} p_{\text{\textsc{dir}}}(\vec{w}_{A_k}\,|\,\vec{\alpha}_{A_k}) \]

In words, the weights of clauses sharing head predicate $A_k$ are drawn from a Dirichlet distribution defined by $\vec{\alpha}_{A_k}$. Sentences $x_j$ are then drawn from the resultant PRCG.

Inference in hierarchical Dirichlet processes in stochastic grammars
and stochastic logic programs (SLPs) has been an active area of
research in recent decades~\cite{DBLP:conf/emnlp/LiangPJK07,
  goldwater2006contextual, johnson2006adaptor, cussens2001parameter}.
Variational inference is a popular approach in this domain and the one
we adopt here. As explained in the next section, we implemented
inference by translating LPNs into PRISM programs and using the
built-in Variational Bayes Expectation-Maximization
algorithm~\cite{sato2008variational}. 

\section{Implementation}

\lstset{
    language=Prolog,
    basicstyle=\fontsize{9}{10.5}\selectfont\ttfamily
}

\begin{figure}[t]
	\centering
	\begin{minipage}[b]{0.8\linewidth}
1) A(a, a) :: 0.25. \\
2) A(b, b) :: 0.3. \\
3) A(X a, Y a) $\leftarrow$ A(X, Y) :: 0.25. \\
4) A(X b, Y b) $\leftarrow$ A(X, Y) :: 0.2.
		\subcaption{}
		\label{fig:grammar}
	\end{minipage}
	%\rule[0pt]{0.8\linewidth}{.5pt}
	\begin{minipage}[b]{0.8\linewidth}
        \fontsize{9}{10.5}\selectfont\ttfamily
		\begin{verbatim}
values(`A', [1, 2, 3, 4],
    [0.25, 0.30, 0.25, 0.20]).

reduce(`A'-[[a],[a]],1).
reduce(`A'-[[b],[b]],2).
reduce(`A'-[A2,B2],3) :- lpn(`A'-[X,Y]),
    append(X,[a],A2), append(Y,[a],B2).
reduce(`A'-[A2,B2],4) :- lpn(`A'-[X,Y]),
    append(X,[b],A2), append(Y,[b],B2).

lpn(P-IN) :- msw(P,V), reduce(P-IN,V).
		\end{verbatim}
		\subcaption{}
		\label{fig:prism}
	\end{minipage}
	\caption{Possible encodings of the 2-copy language as (\subref{fig:grammar}) an LPN, (\subref{fig:prism}) a PRISM program.}
	\label{fig:copy}
\end{figure}

LPNs can be encoded as a restricted and tractable subclass of
SLPs. Specifically, we encode LPNs with PRISM to obtain built-in
predicates for probabilistic execution and Bayesian inference over
SLPs (Figure \ref{fig:copy}). PRISM is an appropriate choice here,
because it implements a previously related extension of the
variational Bayesian Expectation Maximization algorithm for PCFGs to
SLPs satisfying a number of restrictions
\cite{cussens2001parameter}. Our LPN encodings satisfy these
restrictions, in particular the need for a finite explanation graph
({\it i.e.} set of all possible parse trees) for any
utterance. Because we disallow $A_i([\,],[\,])$ as a valid clause,
every term in the body of a clause has shorter arguments than the
head, giving finite derivations and a finite explanation graph.

First, we define the distribution over each non-terminal symbol, $n
\in N$, using the {\tt values} predicate. For each $n$ we define the
values the symbol can take on ({\it i.e.} one for each rule), as well
as the probability of each value. For each rule in the LPN, we also
define a corresponding clause of the {\tt reduce} predicate to capture
the actual relations between the left-hand and right-hand side of each
rule. Sampling and parsing begins with a call to {\tt lpn(P-IN)},
which chooses some rule for the left-hand side via {\tt msw(P,V)} and
recursively reduces the input term using the chosen rule ({\tt
reduce(P-IN,V)}).

\section{Experiments}

To evaluate LPNs as a probabilistic model of concept acquisition, we
trained an LPN with $4$ latent predicates on a set of sentences
expressing successor and predessor relations in numbers between one
and ninety-nine. The training set was the collection of sentences $$X
= \{[\text{after}\, | \, \text{before}] \, \langle n \rangle \,
\text{comes} \, \langle n+1 \rangle \,|\, n \in 1,\dots,99\}.$$ The
lexicon was the set of number word corresponding to $1$ through $19$,
the decades $20, \dots, 30$ and the words ``before'' and ``after.'' It
is not difficult to manually work out an LPN that describes this
limited domain of sentences; see Figure~\ref{fig:parseexample}, for a
possible LPN derivation of an example sentence.

Although it is difficult to know how common these kinds of sentences
are in child-directed speech, words for small numbers are far more
common than words for larger ones(cite CHILDES). On the other hand,
children learning to count to large numbers rehearse the sequence. To
approximate this distribution of evidence, we drew these sample
sentences from a sum of a geometric distribution with parameter $0.5$
and a uniform distribution. These components were weighted $75\%$ and
$25\%$, respectively. We drew $2000$ examples from this distribution,
holding out the sentences in Table~\ref{tab:results} for
evaluation. 

For inference, we used default $\frac{1}{D}$ pseudocounts (where $D$ is the
dimensionality of the Dirichlet distributions). We found that different random
initialization for this experiment did not lead to qualitatively different
results, though further investigation will be necessary to see how
robust our the algorithm is to local maximuma for LPNs.

We evaluated the learned model by asking for Viterbi (i.e. maximum a
posteriori) completions of the last words of each held out test
sentence. Table~\ref{tab:results} shows these completions. The grammar
correctly learns much of the structure of these sentences, including
the difference between sentences starting with ``before'' and
``after'' and the edge cases that relate decade words like ``twenty''
to non-decade words like ``twenty one.''

To inspect visually the learned grammar, we thresholded rules
according to the expected number of times they were used in parsing
the training dataset. Table~\ref{tab:grammar} shows all rules with
expected count above $1e-6$. This reduces from $2669$ to $52$ the
number of significant rules. On inspection, predicate $A_2$ forms
``before'' sentences, predicate $A_4$ forms ``after'' sentences,
predicate $A_3$ is successorship recursively defined over the decades
and ones, and predicate $A_2$ is a category for the decade words.

\begin{table}[t]
\begin{tabular}{>{\footnotesize} l >{\footnotesize} l}
  Question & $K=4$ \\ \hline
  after twenty comes \underline{\hspace{1cm}}? & twenty one \checkmark \\
  after forty five comes \underline{\hspace{1cm}}? & forty six \checkmark \\
  after forty seven comes \underline{\hspace{1cm}}? & forty eight  \checkmark \\
  after forty nine comes \underline{\hspace{1cm}}? & forty ten $\times$ \\
  after fifty nine comes \underline{\hspace{1cm}}? & fifty ten $\times$ \\
  after sixty one comes \underline{\hspace{1cm}}? & sixty two \checkmark \\
  after sixty three comes \underline{\hspace{1cm}}? & sixty four \checkmark \\
  after sixty four comes \underline{\hspace{1cm}}? & sixty five \checkmark \\
  after sixty five comes \underline{\hspace{1cm}}? & sixty six \checkmark \\
  after sixty nine comes \underline{\hspace{1cm}}? & sixty ten $\times$ \\
  after seventy three comes \underline{\hspace{1cm}}? & seventy four \checkmark \\
  after seventy nine comes \underline{\hspace{1cm}}? & seventy ten $\times$ \\
  after ninety five comes \underline{\hspace{1cm}}? & ninety six \checkmark \\
  before twenty three comes \underline{\hspace{1cm}}? & twenty two \checkmark \\
  before thirty comes \underline{\hspace{1cm}}? & thirty eighty $\times$ \\
  before thirty eight comes \underline{\hspace{1cm}}? & thirty seven \checkmark \\
  before forty one comes \underline{\hspace{1cm}}? & forty \checkmark \\
  before fifty three comes \underline{\hspace{1cm}}? & fifty two \checkmark \\
  before sixty eight comes \underline{\hspace{1cm}}? & sixty seven \checkmark \\
  before seventy two comes \underline{\hspace{1cm}}? & seventy one \checkmark \\
  before seventy three comes \underline{\hspace{1cm}}? & seventy two \checkmark \\
  before eighty five comes \underline{\hspace{1cm}}? & eighty four \checkmark \\
  before ninety two comes \underline{\hspace{1cm}}? & ninety one \checkmark \\
  before ninety three comes \underline{\hspace{1cm}}? & ninety two \checkmark \\
  before ninety five comes \underline{\hspace{1cm}}? &ninety four \checkmark \\
\end{tabular}
\caption{results of learning with 4 predicates and XXX}
\label{tab:results}
\end{table}


\begin{figure}[t]
		\includegraphics[width=\linewidth]{parseTree/parse.pdf}
		\caption{A possible parse of the sentence ``after twenty five comes twenty six" using a 5-predicate LPN.}
                \label{fig:parseexample}
\end{figure}

\begin{table}[t]
\begin{tabular}{>{\tiny} l >{\tiny} l}
\multicolumn{2}{>{\tiny}c}{$S$(X Y) $\leftarrow$ $A_1$(X, Y) : 1.0000} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_2$(X, U), $A_3$(V, Y) : 0.5002} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_3$(Y, V), $A_4$(X, U) : 0.3428} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_1$(V, Y), $A_4$(X, U) : 0.0796} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_1$(Y, V), $A_2$(X, U) : 0.0712} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_2$(Y, X), $A_3$(V, U) : 0.0021} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_1$(V, Y), $A_2$(X, U) : 0.0013} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_2$(V, X), $A_3$(Y, U) : 0.0008} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_1$(X, U), $A_2$(V, Y) : 0.0008} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_1$(X, U), $A_2$(Y, V) : 0.0008} \\
\multicolumn{2}{>{\tiny}c}{$A_1$(X Y, U V) $\leftarrow$ $A_1$(X, U), $A_4$(Y, V) : 0.0004} \\
$A_2$(before, comes) : 0.7316 & $A_4$(after, comes) : 0.9990  \\
$A_3$(one, two) : 0.3993 &
$A_3$(two, three) : 0.2063 \\
$A_3$(three, four) : 0.1093 &
$A_3$(four, five) : 0.0734 \\
$A_3$(five, six) : 0.0502 &
$A_3$(six, seven) : 0.0355 \\
$A_3$(eight, nine) : 0.0290 &
$A_3$(seven, eight) : 0.0271 \\
$A_2$(fifty, fifty) : 0.0375 &
$A_2$(thirty, thirty) : 0.0361 \\
$A_2$(eighty, eighty) : 0.0339 &
$A_3$(null, one) : 0.0231 \\
$A_2$(forty, forty) : 0.0332 &
$A_2$(twenty, twenty) : 0.0310 \\
$A_2$(seventy, seventy) : 0.0296 &
$A_2$(sixty, sixty) : 0.0274 \\
$A_2$(ninety, ninety) : 0.0260 &
$A_3$(eighteen, nineteen) : 0.0064 \\
$A_3$(sixteen, seventeen) : 0.0049 &
$A_3$(eleven, twelve) : 0.0044 \\
$A_3$(nine, ten) : 0.0044 &
$A_3$(thirteen, fourteen) : 0.0044 \\
$A_3$(fourteen, fifteen) : 0.0039 &
$A_3$(ten, eleven) : 0.0034 \\
$A_2$(null, fifty) : 0.0043 &
$A_3$(eighty, null) : 0.0030 \\
$A_3$(seventeen, eighteen) : 0.0030 &
$A_3$(nine, sixty) : 0.0025 \\
$A_2$(nine, seventy) : 0.0029 &
$A_3$(nine, forty) : 0.0020 \\
$A_2$(null, thirty) : 0.0022 &
$A_2$(nine, ninety) : 0.0022 \\
$A_3$(twelve, thirteen) : 0.0015 &
$A_3$(fifteen, sixteen) : 0.0015 \\
$A_3$(null, comes) : 0.0010 &
$A_2$(twenty, after) : 0.0014 \\
$A_2$(sixty, before) : 0.0007 &
$A_3$(nineteen, comes) : 0.0005 \\
$A_4$(nine, thirty) : 0.0010 & \\
\end{tabular}
\caption{XXX}
\label{tab:grammar}
\end{table}

\section{Discussion}

LPNs seems particularly well-suited to learning in domains where the
number of concepts is infinite. In domains like these, the only way to
understand each concept is to discover structure in the sentences that
refer to these concepts. While number is a prime example, others
include spatial relations, kinship and other social relations, and
temporal relations. Future work should explore how LPNs might explain
learning in these domains, as well as how more complex number
relations might be learned.

Many of the algorithms and formalisms used here were originally
developed for use in other closely related areas, including logic
programming and semantic parsing. Semantic parsing, in particular,
seems especially related to the challenge we face here of jointly
learning meaning and structure from
sentences~\cite{berant2013semantic, liang2013learning,
  kwiatkowski2010inducing}. Semantic parsing, however, tends to frame
this challenge slightly differently than we do here, namely, by asking
how utterances can be mapped to an explicit internal logical
language. By contrast, we focus here on systems where meaning and
structure seem inseparable. Understanding how these two approaches
relate and inform one another is an interesting and open question.

%% Previous work has shown that generative models of probabilistic
%% programs are a useful tool for understanding human concept learning
%% for domains as varied as handwritten characters and structured visual
%% objects~\cite{dechter2013bootstrap, stuhlmuller2010learning,
%%   lake2012concept}. Here, we continue that work by showing how some
%% forms of both human language learning and concept learning may be
%% fruitfully modeled as inference over a generative model of
%% probabilistic logic programs. Future work should work explore how LPNs
%% and its extensions may apply to other domains where generative models
%% of probabilistic programming have already proven useful, as well as
%% generalizing the current approach to abstract, symbolic concepts.

\bibliographystyle{amsplain}
\bibliography{draft}

\nocite{*}

\end{document}
